# -*- coding: utf-8 -*-
"""multimodal_image_text_similarity_CLIP_LIME.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ygEOYhudLcqiyWSftX1JXaR-go4hMPb
"""

# Install necessary libraries
!pip install transformers lime Pillow wget matplotlib numpy scikit-image

!pip install lime --upgrade

#Clear Cached Images
!rm sample_image.jpg

# Download a sample image from the web using wget
!wget https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Covid_Vaccine_Alters_DNA_Forever_graffito_with_%22Will_it_make_me_an_X-Men%22_addendum%2C_pole%2C_Studio_City%2C_Los_Angeles%2C_California%2C_USA_-_Flickr_-_gruntzooki.jpg/640px-Covid_Vaccine_Alters_DNA_Forever_graffito_with_%22Will_it_make_me_an_X-Men%22_addendum%2C_pole%2C_Studio_City%2C_Los_Angeles%2C_California%2C_USA_-_Flickr_-_gruntzooki.jpg -O sample_image.jpg

#Verify that the image is updated
from PIL import Image
import matplotlib.pyplot as plt

# Open the downloaded image and display it
img = Image.open('sample_image.jpg')
plt.imshow(img)
plt.axis('off')  # Hide axes
plt.show()

# Necessary imports
import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
from lime.lime_image import LimeImageExplainer
from lime.lime_text import LimeTextExplainer
import numpy as np
from skimage.segmentation import mark_boundaries
import matplotlib.pyplot as plt

# Load CLIP model and processor
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Set the model to evaluation mode
model.eval()

# Load and preprocess the image
input_image = Image.open("sample_image.jpg")
input_image_np = np.array(input_image)

# Example input text (replace with your own text)
input_text = "Sharks in Floodwaters"

# Process the inputs (both image and text)
inputs = processor(text=input_text, images=input_image, return_tensors="pt", padding=True)

# Make predictions using the CLIP model
with torch.no_grad():
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image  # Image-text similarity scores
    probs = logits_per_image.softmax(dim=1).cpu().numpy()  # Convert to probabilities

# Output predicted probabilities
print("Predicted probabilities:", probs)

# Initialize LIMEImageExplainer
explainer_image = LimeImageExplainer()

# Define a function to predict probabilities for the image and text
def predict_image_fn(images):
    # You need to specify text along with the image for the CLIP model
    input_texts = ["Sharks in Floodwaters"] * len(images)  # Sample text
    inputs = processor(text=input_texts, images=images, return_tensors="pt", padding=True)

    with torch.no_grad():
        outputs = model(**inputs)
        logits_per_image = outputs.logits_per_image  # Logits for images
        probs = logits_per_image.softmax(dim=1).cpu().numpy()  # Convert to probabilities

    return probs

# Process the image using numpy array
input_image = Image.open("sample_image.jpg")  # Load the sample image
input_image_np = np.array(input_image)

# Explain the image
explanation_image = explainer_image.explain_instance(
    input_image_np,
    predict_image_fn,
    top_labels=2,
    hide_color=0,
    num_samples=1000
)

# Get the image explanation
temp, mask = explanation_image.get_image_and_mask(
    explanation_image.top_labels[0],
    positive_only=False,
    num_features=10,
    hide_rest=False
)

# Display the explanation
plt.imshow(mark_boundaries(temp, mask))
plt.title("Image Explanation")
plt.show()

# ---- LIME for Text Explanation ----

# Initialize LIMETextExplainer
explainer_text = LimeTextExplainer(class_names=['Fake', 'Real'])

# Define a function to predict probabilities for the text
def predict_text_fn(texts):
    input_images = [input_image] * len(texts)  # Use the same image for all text inputs
    inputs = processor(text=texts, images=input_images, return_tensors="pt", padding=True)

    with torch.no_grad():
        outputs = model(**inputs)
        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1).cpu().numpy()

    return probs

# Explain the text
explanation_text = explainer_text.explain_instance(
    input_text,
    predict_text_fn,
    num_features=6
)

# Display the text explanation in the notebook
explanation_text.show_in_notebook(text=True)

# Alternatively, print an explanation summary
print(explanation_text.as_list())  # This will print the list of important text features